---
title: "Preprocessing Notebook"
output: html_document
---

```{r setup, include=FALSE}
# Install and load required packages
# install.packages("tidyverse")
# install.packages("lubridate")
# install.packages("config")
library(tidyverse)
library(lubridate)
library(config)
library(digest)
library(glue)
library(repr)
options(warn=-1)
```


```{r}
# Set options for data frame display
options(tibble.width = Inf)
options(tibble.print_max = 50)
```

# **Functions**

```{r}
convert_to_snake_case <- function(col_name) {
  # Remove '__c' from the column names
  col_name <- gsub('__c', '', col_name)
  
  # Replace periods with underscores
  col_name <- gsub('\\.', '_', col_name)
  
  # Convert camel case to snake case
  # Handle the case where a lowercase letter or digit is followed by an uppercase letter
  col_name <- gsub('([a-z0-9])([A-Z])', '\\1_\\2', col_name)
  
  # Convert the entire string to lowercase
  col_name <- tolower(col_name)
  
  # Replace multiple underscores with a single underscore
  col_name <- gsub('_+', '_', col_name)
  
  return(col_name)
}
```

```{r}
# Define a function to convert entries to datetime
convert_to_datetime <- function(entry) {
  # Convert a given entry into a datetime object using R's as.POSIXct function.
  # Args:
  #   entry (character): The entry to convert to datetime.
  # Returns:
  #   POSIXct: Returns a POSIXct object if conversion is successful, otherwise NA.
  
  tryCatch({
    # Try to parse as a full date
    as.POSIXct(entry, format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
  }, error = function(e) {
    # Check if it's a four-digit number (likely representing a year)
    if (grepl("^\\d{4}$", entry)) {
      as.POSIXct(paste0(entry, "-01-01"), format = "%Y-%m-%d", tz = "UTC")
    } else {
      # If it's neither a full date nor a valid year, return NA or handle as needed
      NA
    }
  })
}
```

```{r}
# Function to check if the given value is a valid Roman numeral
is_roman_numeral <- function(value) {
  # Define a regular expression to match Roman numerals
  roman_pattern <- "^M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$"
  
  # Check if the value matches the Roman numeral pattern
  return(grepl(roman_pattern, value))
}

# Function to check if the given value has a digit
has_digit <- function(value) {
  return(grepl("\\d", as.character(value)))
}

# Function to check if the given value is a Roman numeral or contains a digit
roman_or_numeral <- function(value) {
  if (is_roman_numeral(value) || has_digit(value)) {
    return(TRUE)
  } else {
    return(FALSE)
  }
}
```


```{r}
# Function to find elements in lst1 that are not in lst2
intersection_out <- function(lst1, lst2) {
  lst3 <- lst1[!(lst1 %in% lst2)]
  return(lst3)
}

# Function to find elements in lst1 that are also in lst2
intersection_in <- function(lst1, lst2) {
  lst3 <- lst1[lst1 %in% lst2]
  return(lst3)
}

```


```{r}
# Function to check if a given string `cell` meets specific inclusion and exclusion criteria
# 
# Args:
#   cell (character): The string to check.
#   included_and (character vector, optional): List of keywords; cell must include all of these (case insensitive).
#   included_or (character vector, optional): List of keywords; cell must include at least one of these (case insensitive).
#   excluded_and (character vector, optional): List of keywords; cell must not include any of these (case insensitive).
#   excluded_or (character vector, optional): List of keywords; cell must not include at least one of these (case insensitive).
# 
# Returns:
#   logical: TRUE if `cell` meets all specified criteria, FALSE otherwise.
# 
generalized_contains <- function(cell, included_and=NULL, included_or=NULL, excluded_and=NULL, excluded_or=NULL) {
  if (!is.character(cell)) {
    return(FALSE)  # Return FALSE if `cell` is not a string
  }
  
  cell_lower <- tolower(cell)  # Convert `cell` to lowercase
  
  # Check included_and condition
  if (!is.null(included_and)) {
    if (!all(sapply(included_and, function(keyword) grepl(keyword, cell_lower)))) {
      return(FALSE)  # Return FALSE if `cell_lower` does not contain all keywords in `included_and`
    }
  }
  
  # Check included_or condition
  if (!is.null(included_or)) {
    if (!any(sapply(included_or, function(keyword) grepl(keyword, cell_lower)))) {
      return(FALSE)  # Return FALSE if `cell_lower` does not contain any keyword in `included_or`
    }
  }
  
  # Check excluded_and condition
  if (!is.null(excluded_and)) {
    if (any(sapply(excluded_and, function(keyword) grepl(keyword, cell_lower)))) {
      return(FALSE)  # Return FALSE if `cell_lower` contains any keyword in `excluded_and`
    }
  }
  
  # Check excluded_or condition
  if (!is.null(excluded_or)) {
    if (any(sapply(excluded_or, function(keyword) grepl(keyword, cell_lower)))) {
      return(FALSE)  # Return FALSE if `cell_lower` contains any keyword in `excluded_or`
    }
  }
  
  return(TRUE)  # Return TRUE if all conditions are satisfied
}

### Example usage
cell <- "This board member is a past chairperson."
included_and <- c('board', 'member')
included_or <- c('past', 'former')
excluded_and <- NULL
excluded_or <- NULL
generalized_contains(cell, included_and, included_or, excluded_and, excluded_or)

cell <- "This board member is a past chairperson."
included_and <- c('board', 'member')
included_or <- NULL
excluded_and <- NULL
excluded_or <- c('past', 'former')
generalized_contains(cell, included_and, included_or, excluded_and, excluded_or)

cell <- "prospect_manager_2nd"
included_and <- NULL
included_or <- c('prospect_manager', 'solicitor')
excluded_and <- NULL
excluded_or <- c("2")
generalized_contains(cell, included_and, included_or, excluded_and, excluded_or)
```

```{r}
# Function to pick a value from either col1 or col2 of a given row based on priority rules
# 
# Args:
#   row (data.frame row): The row (data.frame row) from which to pick values.
#   col1 (character): The name of the first column to consider.
#   col2 (character): The name of the second column to consider.
# 
# Returns:
#   object: The value from col1 or col2 based on the priority rules:
#       - If both values are NA, returns NA.
#       - If only one value is NA, returns the non-NA value.
#       - If both values are equal and not NA, returns that value.
#       - Otherwise, returns NA and prints the conflicting values (for debugging).
# 
pick_col <- function(row, col1, col2) {
  x <- row[[col1]]
  y <- row[[col2]]
  
  if (is.na(x) && is.na(y)) {
    return(NA)  # Return NA if both values are NA
  } else if (is.na(x)) {
    return(y)   # Return y if x is NA
  } else if (is.na(y)) {
    return(x)   # Return x if y is NA
  } else if (x == y) {
    return(x)   # Return x (or y, since x == y) if both are equal and not NA
  } else {
    cat(paste(x, y, sep = " "), "\n")  # Print conflicting values for debugging
    return(NA)  # Return NA for conflicting values
  }
}
```

```{r}
# Function to rename the '_merge' column in the merged dataframe to a source indicator column,
# and map its values to more descriptive labels based on merge sources.
# 
# Args:
#   merged_df (data.frame): The merged dataframe resulting from a merge operation.
#   left_source (character): The label for the left dataframe source.
#   right_source (character): The label for the right dataframe source.
# 
# Returns:
#   data.frame: The merged dataframe `merged_df` with the '_merge' column renamed to indicate source,
#               and its values mapped to descriptive labels based on merge sources.
# 
df_merge_source <- function(merged_df, left_source, right_source) {
  # Rename the '_merge' column to 'source'
  new_col <- "df_source"
  while (new_col %in% names(merged_df)) {
    i <- 1
    new_col <- paste0("df_source_", i)
    i <- i + 1
  }
  
  names(merged_df)[names(merged_df) == "_merge"] <- new_col
  
  # Map the source column to more descriptive labels
  source_mapping <- list(
    'left_only' = left_source,
    'right_only' = right_source,
    'both' = paste0(left_source, "/", right_source)
  )
  
  merged_df[[new_col]] <- factor(merged_df[[new_col]], levels = names(source_mapping))
  merged_df[[new_col]] <- factor(merged_df[[new_col]], labels = source_mapping)
  
  return(merged_df)
}
```

```{r}
# Function to remove dollar signs ('$') and commas (',') from a given string representation of a number
# 
# Args:
#   cell (character or any): The string or value from which to remove dollar signs and commas.
# 
# Returns:
#   character or any: The modified string with dollar signs and commas removed, or the original value if not a string.
# 
remove_dollar_sign_and_comma <- function(cell) {
  if (is.character(cell)) {
    cell <- gsub("\\$|,", "", cell)  # Remove dollar signs ('$') and commas (',')
  }
  return(cell)
}
```

```{r}
# Function to save a data frame to a CSV file with a specified file name format and path
# 
# Args:
#   df (data.frame): The data frame to save.
#   file_prefix (character, optional): The prefix for the CSV file name. Default is "constituents_cleaned".
#   version (character, optional): The version identifier for the CSV file name. Default is "v1".
# 
# Returns:
#   NULL
# 
save_file <- function(df, file_prefix = "constituents_cleaned", version = "v1") {
  # Check if "Unique Donor ID" column exists and drop rows with NA values in that column
  if ("Unique Donor ID" %in% names(df)) {
    df <- df[!is.na(df[["Unique Donor ID"]]), ]
  }
  
  # Construct file name based on file_prefix and version
  file_name <- paste0(file_prefix, "_", version, ".csv")
  
  # Define file path components (replace with actual paths as needed)
  file_path <- "/path/to/your/file"  # Replace with actual file path
  clients <- "your_clients_folder"   # Replace with actual clients folder
  subdir <- "your_subdirectory"      # Replace with actual subdirectory
  
  # Create directory structure if it doesn't exist
  dir.create(file.path(file_path, clients, subdir, "Cleaning"), showWarnings = FALSE, recursive = TRUE)
  
  # Save data frame to CSV file
  write.csv(df, file.path(file_path, clients, subdir, "Cleaning", file_name), row.names = FALSE)
}
```

# **Gifts Data**

```{r}
file_path = "C:\\Users\\Rmittal\\CCS\\Internal - Analytics - Shared Drive\\1. Shared Drive\\Clients"
subdir = "Raw Client Data\\Section 2" 
clients = "National Multiple Sclerosis Society"
file_name = "Section 2  CCS May 2024-2024-05-22-20-12-55.csv"
file = "%s\\%s\\%s\\%s" %(file_path, clients, subdir, file_name)
dfg_1 = dplyr::read_csv(file, encoding="ISO-8859-1")

dfg_1 = dfg_1[dfg_1["Stage"]=="Closed Won"]
dfg_1["source"] = "Section 2  CCS May 2024-2024-05-22-20-12-55.csv"
dfg_1["Close Date"] = dplyr::to_lubridate(dfg_1["Close Date"], format="%m/%d/%Y")#, errors = "coerce")

file_name = "Section 2  CCS May 2024 Pt. 2 GAU Ref-2024-05-30-03-00-07.csv"
file = "%s\\%s\\%s\\%s" %(file_path, clients, subdir, file_name)
dfg_2 = dplyr::read_csv(file, encoding="ISO-8859-1")
dfg_2["source"] = "Section 2  CCS May 2024 Pt. 2 GAU Ref-2024-05-30-03-00-07.csv"
dfg_2 = dfg_2.drop(columns="Campaign")

rename_columns={"Opportunity: Opportunity ID": "Opportunity ID", \
               "Opportunity: Opportunity Record Type": "Opportunity Record Type"}

dfg_2.rename(columns=rename_columns, inplace=True)

dfg_2["Close Date"] = dfg_2["Opportunity: Opportunity Name"].str[-10:].values
#dfg_2["Close Date"].head()
dfg_2["Close Date"] = dplyr::to_lubridate(dfg_2["Close Date"], errors = "coerce"))
```

```{r}
#dfg_2[dfg_2["Close Date"].isna()]
dfg_2["Close Date"] = numpy::where((dfg_2["Opportunity ID"]=="006f400000TcYLhAAN") & \
                               (dfg_2["GAU Allocation: ID"]=="a0b5G00000RlpR6QAJ"), \
                               lubridate(2021, 10, 1), dfg_2["Close Date"])
dfg_2["Close Date"] = numpy::where((dfg_2["Opportunity ID"]=="0065G00000XKQIYQA5") & \
                               (dfg_2["GAU Allocation: ID"]=="a0b5G00000TVtxWQAT"), \
                               lubridate(2021, 10, 1), dfg_2["Close Date"])
dfg_2["Close Date"] = numpy::where((dfg_2["Opportunity ID"]=="0065G00000bXpneQAC") & \
                               (dfg_2["GAU Allocation: ID"]=="a0b5G00000VMeCOQA1"), \
                               lubridate(2020, 9, 30), dfg_2["Close Date"])
dfg_2["Close Date"] = numpy::where((dfg_2["Opportunity ID"]=="006f400000TqcZMAAZ") & \
                               (dfg_2["GAU Allocation: ID"]=="a0bPd000000PsEvIAK"), \
                               lubridate(2023, 9, 30), dfg_2["Close Date"])

### Run this again! Ensure the entire column is in lubridate format!
dfg_2["Close Date"] = dplyr::to_lubridate(dfg_2["Close Date"], errors='coerce'))
```

## **Concatenate dfg_1 and dfg_2**

```{r}
dfg = dplyr::concat([dfg_1, dfg_2], axis=0)
dfg["Close Date"] = dplyr::to_lubridate(dfg["Close Date"], errors = "coerce")
dfg["Amount"] = dfg["Amount"].astype(float))
```

```{r}
dfg.rename({"Amount":"gift_amount",\
           "Opportunity Record Type": "gift_type",\
           "Close Date": "gift_date",\
           "Opportunity ID": "gift_id"}, inplace=True))
```

```{r}
save_file(dfg, file_prefix = "gifts_cleaned", version="v1"))
```

## **Five Year Giving**

```{r}
dfg = dfg.rename(columns={"Primary Contact: Contact ID": "constituent_id"})
dfg_subset = dfg[dfg["Close Date"]> lubridate(2019, 10, 1)]
# Example: Drop rows with missing values in 'Amount'
dfg_subset.dropna(subset=["Amount"], inplace=True)
dfg_fyg = dfg_subset.groupby("Opportunity ID")["Amount"].sum().reset_index().rename(columns={"Amount": "five_year_giving"}))
```

# **Combine Files**

```{r}
# Define path and filename variables

file_path = "C:\\Users\\Rmittal\\CCS\\Internal - Analytics - Shared Drive\\1. Shared Drive\\Clients"
subdir = "Raw Client Data\\Section 3" 
clients = "National Multiple Sclerosis Society"
shared_drive =  "%s\\%s\\%s\\" %(file_path, clients, subdir)
#file_names = ["Section 3-A.csv"]

i = 1
for file in os.listdir(shared_drive):
    if "section" in file.lower():
        print(file, i)
        file = "%s\\%s\\%s\\%s" %(file_path, clients, subdir, file)
        Sys.globals()["df_%d" %i] = dplyr::read_csv(file, encoding="ISO-8859-1")
        i += 1)
```

```{r}
run = False
if run:
    df_cd =  dplyr::DataFrame()
    for i in range(1, 12, 1):
        df_cd = dplyr::concat([df_cd, Sys.globals()["df_%d" %i]])
        print(i, Sys.globals()["df_%d" %i].shape, len(df_cd))
        del (Sys.globals()["df_%d" %i]))
```

```{r}
run = False
if run:
    df_cd = dplyr::read_csv("nmss_constit.csv"))
```

# **Convert the column names into snake case**

```{r}
# Apply the convert_to_snake_case function to all column names
df_cd.columns = [convert_to_snake_case(col) for col in df_cd.columns]
#df_cd.columns.to_list())
```

# **Check for overall duplicates**

```{r}
run = False
if run:
    print(len(df_cd))
    df_cd.drop_duplicates(inplace=True)
    len(df_cd)

## 9038462
## 9038462)
```

# **Renaming**

```{r}
rename_columns = {"x18_char_contact_id": "constituent_id",
"account_name": "head_of_household",
"account_id": "household_id",
"account_record_type_name": "key_indicator"}
#Board_Member_Check__c', 'National_Board_Member__c' --> current_trustee_indicator, past_trustee_indicator 
#is_assigned, assigned_manager --> "Account.Owner.Name", "Owner.Name" \
                                  # (list of strings: "migrations user, inactive, integration*")
#Board_Member_Check__c --> we're also checking the file "persona" (csv)
#df_cd.columns.to_list())
```

```{r}
df_cd.rename(columns=rename_columns, inplace=True))
```

# **Constituent_id**

```{r}
len(df_cd), df_cd["constituent_id"].nunique())
```

# **Formatting**

##### **Remove dollar sign and comma**

```{r}
#[(key, df_cd[key][0:5]) for key in df_cd.columns if "amount" in key.lower()])
```

```{r}
run = False
if run:
    amount_columns = [key for key in df_cd.columns if "amount" in key.lower()]
    df_cd[amount_columns] = df_cd[amount_columns].applymap(remove_dollar_sign_and_comma)
    
    for col in amount_columns:
        df_cd[col] = dplyr::to_numeric(df_cd[col], errors='ignore'))
```

# **key_indicator**

```{r}
df_cd["key_indicator"].value_counts())
```

# **is_individual**

```{r}
df_cd["is_individual"] = numpy::where(df_cd["key_indicator"]=="Household", True, False))
```

# **is_deceased**

```{r}
len(df_cd[(df_cd["npsp_deceased"]==False) & ~(df_cd["deceased_date"].isna())][["npsp_deceased", "deceased_date"]]))
```

```{r}
#[(key, df_cd[key].value_counts()) for key in df_cd.columns if "deceased" in key.lower()]
df_cd["is_deceased"] = numpy::where(((df_cd["npsp_deceased"]==True) | (~df_cd["deceased_date"].isna())), True, False)
#df_cd["is_deceased"] = numpy::where(~df_cd["deceased_date"].isna(), True, False)
#df_cd["deceased_date"].isna().sum())
```

# **Trustee**

```{r}
#len(df_cd[(df_cd["board_member_check"]!=1) & (df_cd["national_board_member"]==1)]))
```

```{r}
#[(key, df_cd[key].value_counts()) for key in df_cd.columns if "board" in key.lower()])
```

```{r}
df_cd["current_trustee_indicator"] = numpy::where((df_cd["board_member_check"]==1) | (df_cd["national_board_member"]==1)\
                                               , True, False)
#df_cd["current_trustee_indicator"].value_counts())
```

# **pg_indicator**

```{r}
# Define path and filename variables

file_path = "C:\\Users\\Rmittal\\CCS\\Internal - Analytics - Shared Drive\\1. Shared Drive\\Clients"
subdir = "Raw Client Data" 
clients = "National Multiple Sclerosis Society"
file_name = "Contacts with Personas.csv"

file = "%s\\%s\\%s\\%s" %(file_path, clients, subdir, file_name)
df_pg = dplyr::read_csv(file, encoding="ISO-8859-1")
df_pg.rename(columns={"18 Char Contact ID": "constituent_id"}, inplace=True))
```

```{r}
df_temp = df_pg[df_pg.groupby("constituent_id").transform("size")>1].sort_values(by="constituent_id")
df_temp["Type"].value_counts())
```

```{r}
### Create a subset where Type="Lawry Circle Donor"
df_pg_lawry = df_pg[df_pg["Type"].str.contains("lawry circle donor", case=False, na=False)]

### Create a pg_indicator field in df_cd
pg_constit_id = list(df_pg_lawry["constituent_id"].unique())
df_cd["pg_indicator"] = numpy::where(df_cd["constituent_id"].isin(pg_constit_id), True, False)
#len(pg_constit_id), df_cd["pg_indicator"].value_counts())
```

# **head_of_household and household_id**

# **is_assigned**

```{r}
#df_cd["owner_name"].value_counts()
exclude = ["migration", "inactive", "integration"]
df_cd["is_assigned"] = numpy::where(~df_cd["owner_name"].str.contains("migration|inactive|integration", case=False, na=False), \
                                True, False)
df_cd["is_assigned"].value_counts())
```

```{r}
df = df_cd[0:1000]
df[~df["owner_name"].str.contains("migration|inactive|integration", case=False, na=False)]["owner_name"])
```

# **assigned_manager**

```{r}
# Define conditions
conditions = [
    (df_cd["is_assigned"]==True),
]

# Define choices corresponding to the conditions
choices = [
    df_cd["owner_name"],
]

# Use numpy::select to create the 'personal_email' column
df_cd["assigned_manager"] = numpy::select(conditions, choices, default=numpy::nan))
```

# **solicit_codes**

```{r}
 columns_solicit_codes=['postal_opt_out',
 'do_not_call',
 'et4ae5_has_opted_out_of_mobile',
 'has_opted_out_of_email']
#[(key, df_cd[key].value_counts()) for key in columns_solicit_codes])
```

```{r}
df_cd["solicit_codes"] = numpy::where(df_cd[columns_solicit_codes].any(axis=1), 1, 0)
df_cd["solicit_codes"].value_counts())
```

# **marital_status**

```{r}
df_cd.rename(columns={"marital_status": "marital_status_original"}, inplace=True)
df_cd["marital_status_original"].value_counts())
```

```{r}
df_cd["marital_status"] = numpy::where(df_cd["marital_status_original"]=="Married", 1, 0))
```

# **five_year_giving**

```{r}
run = False
if run:
    df_cd = df_cd.merge(dfg_fyg, on=["constituent_id"], how="left")
#df_cd[[""]].head())
```

```{r}
df_cd["five_year_giving"] = numpy::nan)
```

# **Model Scores**

```{r}
#df_cd["major_donor_model_score"]=random.randint(100, size=(len(df_cd)))
#df_cd["loyalty_model_score"]=random.randint(100, size=(len(df_cd)))

df_cd["major_donor_model_score"] = 0
df_cd["loyalty_model_score"]    = 0

df_cd["n_years_giving_of_ten"] = numpy::nan)
```

# **planned_gift_commitment**

```{r}
#df_cd.columns.to_list()
#columns_planned = [key for key in df_cd.columns if "planned" in key.lower()]
#df_cd["planned_gift_commitment"] = numpy::where(df_cd[columns_planned].notna().any(axis=1), 1, 0))
```

# **Staff Indicator**

```{r}
run = False
if run:
    save_file(df_cd, file_prefix = "constituents_cleaned_no_staff_indicator", version="v2"))
```

```{r}
# Define path and filename variables

file_path = "C:\\Users\\Rmittal\\CCS\\Internal - Analytics - Shared Drive\\1. Shared Drive\\Clients"
subdir = "Raw Client Data" 
clients = "National Multiple Sclerosis Society"
file_name = "All Staff List - for Sara Groh - 05162024.xlsx"

file = "%s\\%s\\%s\\%s" %(file_path, clients, subdir, file_name)
df_staff = dplyr::read_excel(file)
df_staff.rename(columns={"Email - Primary Home": "npe01_home_email",\
                        "Email - Work": "npe01_alternate_email"}, inplace=True))
```

```{r}
#df_staff.head())
```

```{r}
#df_cd_subset = df_cd.dropna(subset=["npe01_home_email", "npe01_alternate_email"])[0:100000])
```

```{r}
#df_cd_subset = df_cd[(~df_cd["npe01_home_email"].isna()) | (~df_cd["npe01_alternate_email"].isna())][0:100000]
df_cd= df_cd.merge(df_staff[["npe01_home_email", "npe01_alternate_email"]],\
                                 on=["npe01_home_email", "npe01_alternate_email"], how="left", indicator=True))
```

```{r}
#df_cd_subset[df_cd_subset["_merge"]=="both"][["npe01_home_email", "npe01_alternate_email"]]
df_cd["staff_indicator"] = numpy::where(df_cd["_merge"]=="both", 1, 0)
#df_cd.drop(columns="_merge", inplace=True))
```

```{r}
df_cd["_merge"].value_counts())
```

```{r}
df_cd["other_street"].value_counts())
```

```{r}
run = False
if run:
    save_file(df_cd, file_prefix="constituents_cleaned", version="v2"))
```

# **Mapper**

```{r}
mapping = dplyr::read_csv("./constit_mapping_nmss.csv")
column_mapping = {row["file_columns"]: row["expected_columns"] \
                  for index, row in mapping.iterrows() if row["file_columns"]!="not_found"}
df_final = df_cd.rename(columns=column_mapping)
df_final = df_final[list(column_mapping.values())]
save_file(df_cd, file_prefix="constituents_cleaned_mapped", version="v1"))
```

# **Address**

```{r}
df_cd["business_address"] = numpy::where(df_cd["preferred_address_type"]=="Business", True, False)
df_cd["seasonal_address"] = numpy::where(df_cd["preferred_address_type"]\
                                    .str.contains("summer|winter|alternate", case=False, na=False), True, False))
```

# **Phone**

```{r}
df_cd["home_phone"] = numpy::where(df_cd["Phone Type"] == "Home", df_cd["Phone Number"], numpy::nan)

# Updating the condition for the second pair of columns
df_cd["home_phone"] = numpy::where(
    (df_cd["Phone Type_1"] == "Home") & df_cd["home_phone"].isna(), 
    df_cd["Phone Number_1"], 
    df_cd["home_phone"]
)

df_cd["cell_phone"] = numpy::where(df_cd["Phone Type"] == "Cell", df_cd["Phone Number"], numpy::nan)

# Updating the condition for the second pair of columns
df_cd["cell_phone"] = numpy::where(
    (df_cd["Phone Type_1"] == "Cell") & df_cd["cell_phone"].isna(), 
    df_cd["Phone Number_1"], 
    df_cd["cell_phone"]
))
```

# **Email**

```{r}
# Define conditions
conditions = [
    df_cd["Email"].notna(),
    df_cd["Email_1"].notna(),
    df_cd["Email_2"].notna(),
    df_cd["Email_3"].notna()
]

# Define choices corresponding to the conditions
choices = [
    df_cd["Email"],
    df_cd["Email_1"],
    df_cd["Email_2"],
    df_cd["Email_3"]
]

# Use numpy::select to create the 'personal_email' column
df_cd["personal_email"] = numpy::select(conditions, choices, default=numpy::nan))
```

# **number_of_events_attended**

```{r}
df_cd["number_of_special_events_attended"] = df_cd[["Events", "Events_1", "Events_2", "Events_3", \
                                                  "Events_4", "Events_5"]].notna().sum(axis=1))
```

# **total_notes**

```{r}
df_cd["Total_Notes"]
df_cd["Total_Notes"] = df_cd["Total_Notes"].fillna(0).astype(int)
df_cd["Total Notes"] = df_cd["Total Notes"].fillna(0).astype(int)
df_cd["total_notes"] = df_cd["Total_Notes"] + df_cd["Total Notes"] )
```

# **Mapper**

### Correlation Plots with Total Lifetime Giving

```{r}
# Drop rows with NaNs in these columns
df_cleaned = df_cdi_multiple.dropna(subset=["Total Lifetime Giving", "Total Actions"])
ggplot2::figure(figsize=(8,2))
ggplot2::scatter(numpy::log(df_cleaned["Total Lifetime Giving"].astype(float)), \
            df_cleaned["Total Actions"].astype(int))
ggplot2::xlabel("Total Lifetime Giving")
ggplot2::ylabel("Total Actions")
ggplot2::title("Scatter Plot of Total Lifetime Giving vs. Total Actions")
ggplot2::show()

df_cleaned = df_cdi_multiple.dropna(subset=["Total Lifetime Giving", "Pledged Planned Gift"])
ggplot2::figure(figsize=(8,2))
ggplot2::scatter(numpy::log(df_cleaned["Total Lifetime Giving"].astype(float)), \
            df_cleaned["Pledged Planned Gift"].astype(float))
ggplot2::xlabel("Total Lifetime Giving")
ggplot2::ylabel("Pledged Planned Gift")
ggplot2::title("Scatter Plot of Total Lifetime Giving vs. Pledged Planned Gift")
ggplot2::show())
```

# **Preprocessing after mapping**

```{r}
df["class_year"] = df["class_year"].str.split("'",expand=True)[1]
df["class_year"] = df["class_year"].fillna(numpy::nan).astype('Int64'))
```

```{r}
mask = ~df["class_year"].isna()
df.loc[mask, "class_year"] = numpy::where((df.loc[mask, "class_year"] > 25),
                                      "19" + df.loc[mask, "class_year"].astype(str),
                                      "20" + df.loc[mask, "class_year"].astype(str)))
```

```{r}
cols_lubridate = list(df.select_dtypes(include=['lubridate']).columns)
for col in cols_lubridate:
    df[col] = dplyr::to_lubridate(df[col]).dt.date)
```

```{r}
)
```